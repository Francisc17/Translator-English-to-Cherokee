{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/franciscomesquita/tp2-ad-translator-english-to-cherokee?scriptVersionId=102879790\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Trabalho Prático 2 - ","metadata":{"id":"WxvRWqBQVzgK"}},{"cell_type":"markdown","source":"Machine Translation - English to Cherokee","metadata":{"id":"bFydYD8rLnnE"}},{"cell_type":"markdown","source":"# Tarefa 1","metadata":{"id":"V9HAR6HiZolj"}},{"cell_type":"code","source":"#Imports importantes\n\n# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\ntry:\n    # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nassert tf.__version__ >= \"2.0\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","metadata":{"id":"DPQTZko6LMsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Conectar ao drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"ZawpF-TrkSGZ","outputId":"9d9fc994-0b53-428a-99a0-727b14979428"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importar dataset\n\nimport pandas as pd\ntrain_dataset = pd.read_csv('/content/drive/MyDrive/Ch_En_Train_complete.csv',delimiter=';')","metadata":{"id":"rlHgzZa6luVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizar dataset de treino\n\ntrain_dataset","metadata":{"id":"u_8MLjYDh-7f","outputId":"cf2fa8b6-c3d2-4bfa-bde8-5bd8251b6bd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng = train_dataset['English']\nche = train_dataset['Cherokee']\n\neng_len = [len(s.split()) for s in eng]\nche_len = [len(s.split()) for s in che]\n\n#Frases com o maior número de palavras para cada linguagem\nmax(eng_len), max(che_len)","metadata":{"id":"WgHbtw9s7DlG","outputId":"49d89555-4afa-4df0-c641-eff9831e8caa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statistics\n\n#Este valor vai ser utilizado como referência para escolher o sequence_length nas camadas de vetorização\n\nstatistics.median(eng_len), statistics.median(che_len)","metadata":{"id":"ZoAc-f_txMLL","outputId":"f9c47514-434b-4e2b-d722-a827a5c0c570"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Número de palavras normal para cada lingua\n\n#Entre 0 e 40 para inglês e 0 e 25 para cherokee\n\n%matplotlib inline\n\nlength_df = pd.DataFrame({'eng':eng_len, 'che':che_len})\nlength_df.hist(bins=30)","metadata":{"id":"iUFF34-a6xNi","outputId":"ea035634-78ff-4f5f-db9f-ff7c6458ec5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_pairs = []\nfor index, row in train_dataset.iterrows():\n    english, ch = row\n    ch = \"[start] \" + ch + \" [end]\"\n    text_pairs.append((english, ch))\n\ntext_pairs","metadata":{"id":"lxanQzDlclF_","outputId":"e0319790-bd37-4339-ceed-8910f30f86ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dividir o datastet em 3 conjuntos: treino, validação e teste\n\nimport random\nrandom.shuffle(text_pairs)\nnum_val_samples = int(0.10 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples:]","metadata":{"id":"Terv66lhM4X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modulo de pré-processamento e vetorização do texto\n\nimport tensorflow as tf\nimport string\nimport re\n\nfrom tensorflow.keras import layers\n\nstrip_chars = string.punctuation\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\n# É necessário criar uma standardização especial para as frases em cherokee, de modo a não retirar os símbolos [ ]\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 8000 #Vai apenas utilizar as X palavras mais utilizadas do vocabulário.\nsequence_length = 20\n\nsource_vectorization = layers.TextVectorization(\n    #comentar linha abaixo para vermos de seguida o nr de tokens criada para cada vocab\n    max_tokens=vocab_size,\n    standardize='lower_and_strip_punctuation', #used by default\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\ntarget_vectorization = layers.TextVectorization(\n    #comentar linha abaixo para vermos de seguida o nr de tokens criada para cada vocab\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length = sequence_length + 1,\n    standardize=custom_standardization,\n)\n\ntrain_english_texts = [pair[0] for pair in train_pairs]\ntrain_ch_texts = [pair[1] for pair in train_pairs]\nsource_vectorization.adapt(train_english_texts)\ntarget_vectorization.adapt(train_ch_texts)","metadata":{"id":"hX1CnrGxPax4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar o vocabulário em inglês\n\nsource_vocab = source_vectorization.get_vocabulary()\nsource_vocab","metadata":{"id":"xY0Eqvx_JRgH","outputId":"e423e2d3-e20a-41a0-ca69-c2f281ccbd75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar o vocabulário em cherokee\ntarget_vocab = target_vectorization.get_vocabulary()\ntarget_vocab","metadata":{"id":"9EfEL9D9Jr2_","outputId":"f7cf4066-6a10-47f7-a749-8b034e7158e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vocabulário se não utilizarmos MaxTokens\n\nplt.bar(['Vocab ENG','Vocab CHE'], [len(source_vocab),len(target_vocab)], color ='maroon',\n        width = 0.4)","metadata":{"id":"40uv1cNA-mVu","outputId":"3a34fb1f-1065-488e-ec8d-6b623fbaab2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vocabulário se utilizarmos MaxTokens\n\nplt.bar(['Vocab ENG','Vocab CHE'], [len(source_vocab),len(target_vocab)], color ='maroon',\n        width = 0.4)","metadata":{"id":"fH_eoQmfAZZn","outputId":"fb7cb5bd-9896-4119-bca1-bc2753c71576"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a data pipeline to feed into the translator model\n# Utiliza o objeto tf.data\n\nbatch_size = 64\n\ndef format_dataset(eng, ch):\n    eng = source_vectorization(eng)\n    ch = target_vectorization(ch)\n    return ({\n        \"english\": eng,\n        \"cherokee\": ch[:, :-1],\n    }, ch[:, 1:])\n\ndef make_dataset(pairs):\n    eng_texts, ch_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    ch_texts = list(ch_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ch_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","metadata":{"id":"yjNBmkmaQE1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizar o formato e alguns exemplos\n\nfor inputs, targets in train_ds.take(1):\n    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n    print(f\"inputs['cherokee'].shape: {inputs['cherokee'].shape}\")\n    print(f\"targets.shape: {targets.shape}\")\n    print(inputs['english'])\n","metadata":{"id":"Ptr32zceQLUx","outputId":"26820d95-b06b-4caa-8c4b-dd53eb91a606"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criação da classe que modela o Encoder \n\n# Na criação do objeto recebe \n# embed_dim: Dimensão da sequência de input \n# dense_dim: Número de nós da camada Dense\n# num_heads: Número de attention heads\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[:, tf.newaxis, :]\n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config","metadata":{"id":"7r7M3lbPTCYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criação da classe que modela o Decoder \n\n# Na criação do objeto recebe \n# embed_dim: Dimensão da sequência de input \n# dense_dim: Número de nós da camada Dense\n# num_heads: Número de attention heads\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"),\n             layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim,\n        })\n        return config\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1),\n             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n        return tf.tile(mask, mult)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(\n                mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=causal_mask)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        attention_output_2 = self.layernorm_2(\n            attention_output_1 + attention_output_2)\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)","metadata":{"id":"1jFFDfMoQdER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n            \"input_dim\": self.input_dim,\n        })\n        return config","metadata":{"id":"4jfXOf3nTMOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The complete Transformer\n\nimport keras\n\n# Settings \n\nembed_dim = 256\ndense_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"cherokee\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n\ntransformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"id":"JW_VGzs1TPcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definir o callback para o modelo parar de treinar se detetar situação de overfitting\nearly_stopping_cb = keras.callbacks.EarlyStopping(\n    patience=8, restore_best_weights=True)\n\n#Compilar o transformer\ntransformer.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\n#Treinar o transformer\ntransformer.fit(train_ds, epochs=40, validation_data=val_ds, callbacks=[early_stopping_cb])","metadata":{"id":"FdhUT_gqTX8w","outputId":"eaa39cf3-f5b6-40f2-cd22-8b950c75313f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Bleu e Sacrebleu metrics (Vai ser usado Bleu Score e CHRF)\n#Ref Bleu: https://towardsdatascience.com/nlp-metrics-made-simple-the-bleu-score-b06b14fbdbc1\n#Ref Sacrebleu - CHRF: https://huggingface.co/spaces/evaluate-metric/chrf\n\nfrom nltk.translate.bleu_score import sentence_bleu\n!pip install sacrebleu\nfrom sacrebleu.metrics import BLEU, CHRF, TER\n","metadata":{"id":"NOwIBQFqKGCg","outputId":"a6daaf4e-5f09-453c-87d2-0836d0875219"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu = BLEU()\nchrf = CHRF()","metadata":{"id":"C7PMb_N3Vsva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testar o desempenho do Transformer em frases do conjunto de teste\n# Métricas Bleu e CHRF são utilizadas\n\nimport numpy as np\nch_vocab = target_vectorization.get_vocabulary()\nch_index_lookup = dict(zip(range(len(ch_vocab)), ch_vocab))\nmax_decoded_sentence_length = 20\nmedium_Bleu_Score = 0\nmedium_chrf_Score = 0\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization(\n            [decoded_sentence])[:, :-1]\n        predictions = transformer(\n            [tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = ch_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n#Teste do modelo\nfor _ in range(50):\n    test = random.choice(test_pairs)\n    input_sentence = test[0]\n    translation = decode_sequence(input_sentence)\n    ref = [test[1]]\n    score_bleu = sentence_bleu(ref, translation)\n    score_chrf = str(chrf.corpus_score([translation],[ref]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    medium_Bleu_Score += score_bleu\n    medium_chrf_Score += score_chrf\n    print(f\"Frase a traduzir: {input_sentence}\")    \n    print(\"previsão: \" + decode_sequence(input_sentence))\n    print(\"Real: \" + ref[0])\n    print(\"Bleu Score: \" + str(score_bleu))\n    print(\"CHRF Score: \" + str(score_chrf) + '\\n')\n\nprint(\"Média final de Bleu Score: \" + str(medium_Bleu_Score / 50))\nprint(\"Média final de CHRF Score: \" + str(medium_chrf_Score / 50))","metadata":{"id":"_h6nkceEYpUH","outputId":"5a503410-b4ce-48cc-c0a5-505e36c76e6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testar o desempenho do transformer em frases introduzidas pelo utilizador\n# Especificar 5 frases em inglês e verificar a qualidade da tradução\n\nfor _ in range(5):\n    input_sentence = input()\n    translation = decode_sequence(input_sentence)\n    print(\"-\")\n    print(input_sentence)\n    print(f\"Frase traduzida: {translation}\")","metadata":{"id":"UONQEruoZIyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tarefa 2\n\nUtilizar modelos pré-treinados para traduzir","metadata":{"id":"MVLb4aGgaZ-N"}},{"cell_type":"markdown","source":"## 1ª Abordagem - Bert pre-trained model\n\n\n\nBaseado no código presente em: https://github.com/vivekgohel56/Neural-machine-translation-english-to-polish\n\nFoi uma tentativa de implementar uma arquitetura referenciada na área de NLP e adaptar ao nosso problema, porém não tivemos em consideração o elevado número de parâmetros (iriamos reparar depois ao treinar)","metadata":{"id":"E919SQ0weI3N"}},{"cell_type":"code","source":"#Veersão 2.2.0 é necessária devida a certas especificações de bibliotecas e respetivas funções usadas abaixo\n\n!pip uninstall tensorflow --yes\n!pip install tensorflow==2.2.0","metadata":{"id":"nG-cMlh-uRyT","outputId":"e594f8dc-7bd3-4cd3-d6e4-da04731b327b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instalar pacotes e importar libraries\n\n!pip install bert-for-tf2\n!pip install numpy==1.19.5\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, load_stock_weights","metadata":{"id":"i6oa6f_mD8Jt","outputId":"8215b258-f06c-4f64-e95a-1e294d854847"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Conectar ao drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"o68pqzj1FQ6O","outputId":"2f187047-fbfb-4dc3-a6d1-661e369fc785"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Download do modelo se não existir\n#https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n\nif not os.path.exists('/content/drive/My Drive/machine translation/uncased_L-12_H-768_A-12'):\n  !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip \n  !unzip uncased_L-12_H-768_A-12 ","metadata":{"id":"6hiIkxSSgim5","outputId":"93ffb271-4304-481b-8f3c-cadfe31b0a9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uncleaned_data_list = pd.read_csv('/content/drive/MyDrive/Ch_En_Train_complete.csv',delimiter=';')\n\ntext_pairs = []\nfor index, row in uncleaned_data_list.iterrows():\n    english, port = row\n    text_pairs.append((english, port))\n\n\nenglish_word  = [pair[0] for pair in text_pairs]\ncherokee_word = [pair[1] for pair in text_pairs]","metadata":{"id":"7Z7vNmqAgmb_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame(columns=['English','Cherokee'])\ndata['English'] = english_word\ndata['Cherokee'] = cherokee_word","metadata":{"id":"ciHIILA7jQCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"IzK3cA_PjYyJ","outputId":"899fa0b2-47cc-448b-ee6d-fb28f7e90196"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vamos utilizar 80% para treino e o restante para teste\ntrain = int(len(data)*0.8)\ntest = len(data) - train\ntrain_examples, val_examples = data.iloc[0:train,:], data.iloc[train:len(data),:]","metadata":{"id":"rylBEJI6jjFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#criar listas necessárias para treino e teste\n\nenglish_text = train_examples['English'].values\ncherokee_text = train_examples['Cherokee'].values\nenglish_val_text = val_examples['English'].values\ncherokee_val_text = val_examples['Cherokee'].values","metadata":{"id":"luoTIMSwjnOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transformar para datasets do tensorflow\n\ntrain_examples = tf.data.Dataset.from_tensor_slices((english_text, cherokee_text))\nval_examples = tf.data.Dataset.from_tensor_slices((english_val_text, cherokee_val_text))","metadata":{"id":"xHNOn3wFj9jl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_examples)","metadata":{"id":"eHvOXJEjkDiF","outputId":"5be1461a-6422-41f0-a7cb-97920ab0f202"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_examples)","metadata":{"id":"h4zkKrjhkHRZ","outputId":"d5ba9977-2f66-4020-c816-d6d864926c66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exemplo\n\nfor en, ch in train_examples.take(1):\n  print(tf.compat.as_text(en.numpy()))\n  print(tf.compat.as_text(ch.numpy()))","metadata":{"id":"ZT3YGrlqkRzh","outputId":"adf10226-dfa7-4ca2-dbb2-fdf1c804411f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport unicodedata\n\n#Diferentes funções utilizadas no pré-processamento\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(\"utf-8\", \"ignore\")\n    else:\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n    \n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n    \n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n    \n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n    \n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n    \n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n    \n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(\"C\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False","metadata":{"id":"2o89-3AmkXQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install tensorflow-datasets\nimport tensorflow_datasets as tfds\n\n#Vai ler o vocabulário de um ficheiro caso exista, senão vai criar a partir dos dados e guardar num ficheiro (no nosso caso aplica-se a segunda opção)\n#Depois de obtido o vocabulário, este vai ser tokenizado - pode ser visto isto no print\n#Isto é o procedimento feito para cherokee\n\nvocab_file = '/content/drive/My Drive/machine translation/vocab_ch.txt'\nif os.path.isfile(vocab_file + '.subwords'):\n  tokenizer_ch = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_file)\nelse: \n  tokenizer_ch = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n      (cherokee_text), target_vocab_size=2 ** 13)\n  tokenizer_ch.save_to_file('vocab_ch.txt')\n\nsample_string = 'Transformer jest niesamowity.'\ntokenized_string = tokenizer_ch.encode(sample_string)\nfor ts in tokenized_string:\n  print ('{} ----> {}'.format(ts, tokenizer_ch.decode([ts])))","metadata":{"id":"q0QroFSkknU5","outputId":"05a071c1-4b48-4b0c-c032-3c729cba5941"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#O tokenizer para inglês é criado a partir do vocab presente no bert model\n\ntokenizer_en = FullTokenizer(\n    vocab_file= 'uncased_L-12_H-768_A-12/vocab.txt',\n    do_lower_case=True)\n\ntest_tokens = tokenizer_en.tokenize(english_text[-1])\ntest_ids = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + test_tokens + ['[SEP]'])\nprint(test_ids)\nprint(tokenizer_en.convert_ids_to_tokens(test_ids))","metadata":{"id":"80z7AZM7nczk","outputId":"88f10795-a8e2-429b-ed64-314391b2babe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definição do encoder onde uma sequência de palavras pode ter no máximo 50 palavras\n\nMAX_SEQ_LENGTH = 50\n\n\ndef encode(en, ch, seq_length=MAX_SEQ_LENGTH):\n  tokens_en = tokenizer_en.tokenize(tf.compat.as_text(en.numpy()))\n  lang1 = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n  if len(lang1)<seq_length:\n    lang1 = lang1 + list(np.zeros(seq_length - len(lang1), 'int32'))\n\n  lang2 = [tokenizer_ch.vocab_size] + tokenizer_ch.encode(tf.compat.as_text(ch.numpy())) + [tokenizer_ch.vocab_size + 1]\n  if len(lang2)<seq_length:\n    lang2 = lang2 + list(np.zeros(seq_length - len(lang2), 'int32'))\n\n  return lang1, lang2","metadata":{"id":"Bkal6icmpzDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encode function que vai chamar a função acima\n\ndef tf_encode(en, ch):\n  result_en, result_ch = tf.py_function(encode, [en, ch], [tf.int32, tf.int32])\n  result_en.set_shape([None])\n  result_ch.set_shape([None])\n\n  return result_en, result_ch","metadata":{"id":"z9PF53ATp9Vi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filtro para selecioanr as frases(sequências de palavras) que têm abaixo de MAX_SEQ_LENGTH. \n\ndef filter_max_length(x, y, max_length=MAX_SEQ_LENGTH):\n  return tf.logical_and(tf.size(x) <= max_length,\n                        tf.size(y) <= max_length)","metadata":{"id":"q_johGI9qEUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar os dataset já com pré processamento realizado.\n\nBUFFER_SIZE = 40000\nBATCH_SIZE = 64\n\ntrain_dataset = train_examples.map(tf_encode)\n# train_dataset = tf.io.decode_raw(train_dataset, tf.int32)\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# cache the dataset to memory to get a speedup while reading from it.\ntrain_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n    BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\nval_dataset = val_examples.map(\n    lambda en, ch: tf.py_function(encode, [en, ch], [tf.int32, tf.int32]))\nval_dataset = val_dataset.filter(filter_max_length)\nval_dataset = val_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))","metadata":{"id":"nKX6tu_SqPS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Funçaõ acessória para o encoding posicional\n\ndef get_angles(pos, i, d_model):\n  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n  return pos * angle_rates","metadata":{"id":"djLmLs3OqXNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"codificação posicional\" é adicionada para fornecer ao modelo algumas informações sobre a posição relativa dos tokens na frase.\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n\n    # apply sin to even indices in the array; 2i\n    sines = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    cosines = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n\n    pos_encoding = pos_encoding[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"id":"e5jVZe1Uqbty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO - Isto foi acrescentado - necessário testar!\n\n#Apenas um gráfico representativo do encoding posicional\n\n# pós adicionar a codificação posicional, os tokens ficarão mais próximos uns \n# dos outros com base na similaridade de seu significado e sua posição na sentença , no espaço d-dimensional.\n\nn, d = 2048, 512\npos_encoding = positional_encoding(n, d)\nprint(pos_encoding.shape)\npos_encoding = pos_encoding[0]\n\n# Juggle the dimensions for the plot\npos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\npos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\npos_encoding = tf.reshape(pos_encoding, (d, n))\n\nplt.pcolormesh(pos_encoding, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"id":"TrHaZpIXLdjI","outputId":"230d8385-b028-46c6-fdaa-9ff64a82333b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar uma máscara para ser aplicada nos tokens\n\ndef create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # add extra dimensions so that we can add the padding\n  # to the attention logits.\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","metadata":{"id":"Q6zWSHf-qdxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar uma máscara por antecipação - usada para mascarar os tokens futuros em uma sequência. Em outras palavras, a máscara indica quais entradas não devem ser usadas.\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)","metadata":{"id":"y_f74knZqf-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Função utilizada para calcular os pesos da atenção. Tem três entradas: Q (consulta), K (chave), V (valor).\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead) \n    but it must be broadcastable for addition.\n    \n    Args:\n      q: query shape == (..., seq_len_q, depth)\n      k: key shape == (..., seq_len_k, depth)\n      v: value shape == (..., seq_len_v, depth_v)\n      mask: Float tensor with shape broadcastable \n            to (..., seq_len_q, seq_len_k). Defaults to None.\n      \n    Returns:\n      output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights","metadata":{"id":"XGcx4nqgqkN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cria um sistema de atenção multi-head -  permite que o modelo atenda conjuntamente a informações de diferentes subespaços de representação em diferentes posições.\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention,\n                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights","metadata":{"id":"Q_bv3TOKqlvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Em cada local na sequência, y , o MultiHeadAttention executa todas as 8 cabeças de atenção em todos os outros locais da sequência, \n#retornando um novo vetor de mesmo comprimento em cada local.\n\ntemp_mha = MultiHeadAttention(d_model=512, num_heads=8)\ny = tf.random.uniform((1, 60, 768))  # (batch_size, encoder_sequence, d_model)\nq = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\nout, attn = temp_mha(y, k=y, q=q, mask=None)\nout.shape, attn.shape","metadata":{"id":"rpc25Vz4qo3L","outputId":"30840c2f-20ba-4426-d032-e3a5c1025168"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A rede de feed forward pontual consiste em duas camadas totalmente conectadas com uma ativação ReLU entre elas.\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])","metadata":{"id":"JL69Q8b1q4wl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Função para construir o encoder\n\ndef build_encoder(config_file):\n    with tf.io.gfile.GFile(config_file, \"r\") as reader:\n        stock_params = StockBertConfig.from_json_string(reader.read())\n        bert_params = stock_params.to_bert_model_layer_params()\n\n    return BertModelLayer.from_params(bert_params, name=\"bert\")","metadata":{"id":"33LAqWiHq8Dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Camada do codificador - cada camada tem atenção multi-cabeça com mascaramento e redes feed forward pontuais\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"id":"5jbtiY_Lq8za"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_decoder_layer = DecoderLayer(512, 8, 2048)\nsample_encoder_output = tf.random.uniform((64, 128, 768))\n\nsample_decoder_layer_output, _, _ = sample_decoder_layer(\n    tf.random.uniform((64, 50, 512)), sample_encoder_output,\n    False, None, None)\n\nsample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)","metadata":{"id":"HIWIoIgCq_tz","outputId":"cb4f62d5-8223-4dd3-e473-e5d88af47b0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Camada do decodificador - camada consiste em várias sub-camadas: \n#Atenção de várias cabeças mascaradas (com máscara de antecipação e máscara de preenchimento)\n#Atenção multi-cabeça (com máscara de preenchimento).\n#Redes de feed forward pontuais\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n                 rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights","metadata":{"id":"gLgpULdcrCST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nsample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, target_vocab_size=8000)\n\noutput, attn = sample_decoder(tf.random.uniform((64, 26)), \n                              enc_output=sample_encoder_output, \n                              training=False, look_ahead_mask=None, \n                              padding_mask=None)\n\noutput.shape, attn['decoder_layer2_block2'].shape","metadata":{"id":"wC5KvxHcrF6_","outputId":"79d75916-f592-4efa-8a3f-7e54f4dc21d8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar uma classe para configuração que é depois usada para definir particularidades do transformer\n\nclass Config(object):\n  def __init__(self, num_layers, d_model, dff, num_heads):\n    self.num_layers = num_layers\n    self.d_model = d_model\n    self.dff = dff\n    self.num_heads= num_heads","metadata":{"id":"iHjPEEN4rGvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar o transformer\n\nfrom bert.loader import map_to_stock_variable_name\n# /content/drive/My Drive/machine translation/transformer/bert\nclass Transformer(tf.keras.Model):\n  def __init__(self, config,\n               target_vocab_size, \n               bert_config_file,\n               bert_training=False, \n               rate=0.1,\n               name='transformer'):\n      super(Transformer, self).__init__(name=name)\n\n      self.encoder = build_encoder(config_file=bert_config_file)\n      self.encoder.trainable = bert_training\n\n      self.decoder = Decoder(config.num_layers, config.d_model, \n                             config.num_heads, config.dff, target_vocab_size, rate)\n\n      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  #Carregar os pesos\n  def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n      assert isinstance(bert, BertModelLayer), \"Expecting a BertModelLayer instance as first argument\"\n      assert tf.compat.v1.train.checkpoint_exists(ckpt_file), \"Checkpoint does not exist: {}\".format(ckpt_file)\n      ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n\n      bert_prefix = 'transformer/bert'\n\n      weights = []\n      for weight in bert.weights:\n          stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n          if ckpt_reader.has_tensor(stock_name):\n              value = ckpt_reader.get_tensor(stock_name)\n              weights.append(value)\n          else:\n              raise ValueError(\"No value for:[{}], i.e.:[{}] in:[{}]\".format(weight.name, stock_name, ckpt_file))\n      bert.set_weights(weights)\n      print(\"Done loading {} BERT weights from: {} into {} (prefix:{})\".format(\n          len(weights), ckpt_file, bert, bert_prefix))\n\n  def restore_encoder(self, bert_ckpt_file):\n      # loading the original pre-trained weights into the BERT layer:\n      self.load_stock_weights(self.encoder, bert_ckpt_file)\n\n  def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n      enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n\n      # dec_output.shape == (batch_size, tar_seq_len, d_model)\n      dec_output, attention_weights = self.decoder(\n          tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n      return final_output, attention_weights","metadata":{"id":"VpaQ5_TOrKdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_vocab_size = tokenizer_ch.vocab_size + 2\ndropout_rate = 0.15\nconfig = Config(num_layers=6, d_model=512, dff=1024, num_heads=8)","metadata":{"id":"1yPfQtHprNCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gs_folder_bert\n# uncased_L-12_H-768_A-12\nMODEL_DIR = \"uncased_L-12_H-768_A-12\"\nbert_config_file = os.path.join(MODEL_DIR, \"bert_config.json\")\nbert_ckpt_file = os.path.join(MODEL_DIR, 'bert_model.ckpt')\n\n# with tpu_strategy.scope():\ntransformer = Transformer(config=config,\n                          target_vocab_size=target_vocab_size,\n                          bert_config_file=bert_config_file)\n  \ninp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\ntar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\nprint(tar_inp.shape) # (batch_size, tar_seq_len) \n\nfn_out, _ = transformer(inp, tar_inp, \n                        True,\n                        look_ahead_mask=None,\n                        dec_padding_mask=None)\nprint(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n\n# init bert pre-trained weights\ntransformer.restore_encoder(bert_ckpt_file)","metadata":{"id":"DmJhTB6arQm0","outputId":"a954bde7-1072-42ac-8767-e0388e723f56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sumário do modelo - (Aqui apercebo-nos que seria impossível treinar o modelo no Colab em tempo útil)\n\ntransformer.summary()","metadata":{"id":"g6YkdKCC-Nni","outputId":"dc71cf41-a465-4fd4-da8d-df54ca9bb355"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","metadata":{"id":"LsjHoQfz-PRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = CustomSchedule(config.d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)","metadata":{"id":"cnXCJvE6-Rmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Seleção do Learning Rate ideal utilizando a função schedule\n\ntemp_learning_rate_schedule = CustomSchedule(config.d_model)\nimport matplotlib.pyplot as plt\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","metadata":{"id":"Wb3gCGdj-VFU","outputId":"a72268be-9a6e-4315-b320-4f7ea4fb401f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definição da lossobject que vai ser utilizada já abaixo para criar a loss function\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","metadata":{"id":"sD6fR7Hh-WZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criação da função loss que vai ser usada depois durante o treino do modelo\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","metadata":{"id":"KhmO850T-YQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definição das métricas utilizadas durante o treino\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n    name='train_accuracy')","metadata":{"id":"iztc1FXW-arE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar checkpoints de treino (Muito útil devido ao longo tempo que pode demorar e assim o progresso não é perdido!)\n\ncheckpoint_path = \"/content/drive/My Drive/machine translation/checkpoints/train\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print('Latest checkpoint restored!!')","metadata":{"id":"SqIIT2ou-cW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Função para criar os mascaras para os tokens (tanto inputs como outputs)\n\ndef create_masks(inp, tar):\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by \n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return combined_mask, dec_padding_mask","metadata":{"id":"BDPziwtM-ea6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#definição de aquilo que vai ser um step de treino, o que será a loss function utilizada assim como as métricas\n\n@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    \n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp, \n                                     True,\n                                     combined_mask,\n                                     dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)","metadata":{"id":"kdDJNiyq-gj0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IMPOSSÍVEL TREINAR**\n\nSendo um modelo com um valor enormíssimo de parâmetros é computacionalmente demasiado exigente de implementar.\n\n**Infelizmente não foi possível treinar o modelo porém ficamos com uma visão de como funciona a famosa arquitetura BERT**","metadata":{"id":"gbSODK6R8vS8"}},{"cell_type":"code","source":"#Treino durante 11 EPOCHS\n\nimport time\n\nEPOCHS = 11\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> chinese, tar -> english\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar)\n\n        if batch % 500 == 0:\n            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n\n    if (epoch + 1) % 1 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1,\n                                                            ckpt_save_path))\n\n    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n                                                        train_loss.result(),\n                                                        train_accuracy.result()))\n\n    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","metadata":{"id":"xmOAZf4u-hbY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n\n\n\n\n\n","metadata":{"id":"ZmrsJw4NQmXG"}},{"cell_type":"markdown","source":"## 2ª Abordagem - Transformer mais simples","metadata":{"id":"ja_L5-lc9xuA"}},{"cell_type":"markdown","source":"**Não consegue traduzir praticamente nada**\n\nProvavelmente o modelo não é indicado para este problema\nA baixa quantidade de dados influencia bastante para a fraca ou quase nula tradução","metadata":{"id":"kC-sG4IsBWKs"}},{"cell_type":"code","source":"#Library utilizada:  https://simpletransformers.ai/docs/seq2seq-model/\n!pip install simpletransformers","metadata":{"id":"ixEhqd6sM9Nh","outputId":"6cab27b9-bbfe-4e8f-aac7-2f47ba834078","execution":{"iopub.status.busy":"2022-06-26T12:03:20.911397Z","iopub.execute_input":"2022-06-26T12:03:20.912273Z","iopub.status.idle":"2022-06-26T12:03:44.774817Z","shell.execute_reply.started":"2022-06-26T12:03:20.912174Z","shell.execute_reply":"2022-06-26T12:03:44.77355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"pWlCosqQ1tdF","outputId":"ef7c2b54-52a5-43dc-dba6-3df986350c48","execution":{"iopub.status.busy":"2022-06-26T12:03:44.778094Z","iopub.execute_input":"2022-06-26T12:03:44.778744Z","iopub.status.idle":"2022-06-26T12:03:44.848166Z","shell.execute_reply.started":"2022-06-26T12:03:44.778696Z","shell.execute_reply":"2022-06-26T12:03:44.84665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/en-ch-translation/Ch_En_Train - complete.csv',delimiter=';')\nClean = True","metadata":{"id":"Wl0DahxoMHqt","execution":{"iopub.status.busy":"2022-06-26T12:06:05.243904Z","iopub.execute_input":"2022-06-26T12:06:05.24464Z","iopub.status.idle":"2022-06-26T12:06:05.393891Z","shell.execute_reply.started":"2022-06-26T12:06:05.244598Z","shell.execute_reply":"2022-06-26T12:06:05.392803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pré processamento utilizado.\n\nimport re\nimport string\n\nif Clean:\n    # converting every letter to lower case\n    \n    #Não se deve utilizar aqui lower para cherokee! (Penso que não exista caracteres maiusculos e minusculos em cherokee)\n    #df[\"Cherokee\"] = df[\"Cherokee\"].apply(lambda x: str(x).lower())\n    df[\"English\"] = df[\"English\"].apply(lambda x: str(x).lower())\n\n    # removing apostrophe from the sentences\n    df[\"Cherokee\"] = df[\"Cherokee\"].apply(lambda x: re.sub(\"'\", \"\", x))\n    df[\"English\"] = df[\"English\"].apply(lambda x: re.sub(\"'\", \"\", x))\n    exclude = set(string.punctuation)\n    \n    # removing all the punctuations\n    df[\"Cherokee\"] = df[\"Cherokee\"].apply(\n        lambda x: \"\".join(ch for ch in x if ch not in exclude)\n    )\n    df[\"English\"] = df[\"English\"].apply(\n        lambda x: \"\".join(ch for ch in x if ch not in exclude)\n    )\n   \n    # removing digits from the sentences\n    digit = str.maketrans(\"\", \"\", string.digits)\n    df[\"Cherokee\"]= df[\"Cherokee\"].apply(lambda x: x.translate(digit))\n    df[\"English\"] = df[\"English\"].apply(lambda x: x.translate(digit))","metadata":{"id":"MEyO9C5dPhQ0","execution":{"iopub.status.busy":"2022-06-26T12:06:08.812807Z","iopub.execute_input":"2022-06-26T12:06:08.813174Z","iopub.status.idle":"2022-06-26T12:06:09.448559Z","shell.execute_reply.started":"2022-06-26T12:06:08.813134Z","shell.execute_reply":"2022-06-26T12:06:09.447575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"li_2T3loQbUT","outputId":"9cdb03c3-90f5-4e6f-8eed-694a13a9c98b","execution":{"iopub.status.busy":"2022-06-26T12:06:16.661553Z","iopub.execute_input":"2022-06-26T12:06:16.661934Z","iopub.status.idle":"2022-06-26T12:06:16.683987Z","shell.execute_reply.started":"2022-06-26T12:06:16.661902Z","shell.execute_reply":"2022-06-26T12:06:16.683156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar os vários datasets e preparar para fornecer ao modelo\n\ntext_pairs = []\nfor index, row in df.iterrows():\n    english, ch = row\n    ch = \"[start] \" + ch + \" [end]\"\n    text_pairs.append((english, ch))\n\nimport random\nrandom.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples:]\n\ntrain_df=pd.DataFrame(train_pairs)\nval_df = pd.DataFrame(val_pairs)\ntest_df = pd.DataFrame(test_pairs)\n\ntrain_df.columns = ['input_text','target_text']\nval_df.columns = ['input_text','target_text']\ntest_df.columns = ['input_text','target_text']  \n\n","metadata":{"id":"2--FbG8NsF6T","execution":{"iopub.status.busy":"2022-06-26T12:06:20.45702Z","iopub.execute_input":"2022-06-26T12:06:20.457638Z","iopub.status.idle":"2022-06-26T12:06:21.291714Z","shell.execute_reply.started":"2022-06-26T12:06:20.457601Z","shell.execute_reply":"2022-06-26T12:06:21.290732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"id":"hY7Z0wrKQn13","outputId":"0fa5abe2-42ad-4d11-d540-bcac2400794d","execution":{"iopub.status.busy":"2022-06-26T12:06:27.590288Z","iopub.execute_input":"2022-06-26T12:06:27.590643Z","iopub.status.idle":"2022-06-26T12:06:27.602945Z","shell.execute_reply.started":"2022-06-26T12:06:27.590613Z","shell.execute_reply":"2022-06-26T12:06:27.601756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports necessários\n\nimport logging\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom simpletransformers.seq2seq import Seq2SeqModel,Seq2SeqArgs\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","metadata":{"id":"zg1YuSPPpdCW","execution":{"iopub.status.busy":"2022-06-26T12:06:31.412928Z","iopub.execute_input":"2022-06-26T12:06:31.413307Z","iopub.status.idle":"2022-06-26T12:06:39.563704Z","shell.execute_reply.started":"2022-06-26T12:06:31.413274Z","shell.execute_reply":"2022-06-26T12:06:39.562051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definição de hiperparametros do modelo\n#Do que foi testado por nós não notamos qualquer alteração na performance do modelo ao alterar estes hiper parametros\n\nmodel_args = Seq2SeqArgs()\nmodel_args.num_train_epochs = 30\nmodel_args.no_save = True\nmodel_args.evaluate_generated_text = False\nmodel_args.evaluate_during_training = False\nmodel_args.evaluate_during_training_verbose = True\nmodel_args.rag_embed_batch_size = 32\nmodel_args.max_length = 120\nmodel_args.src_lang =\"en\"\nmodel_args.tgt_lang =\"ch\"\nmodel_args.overwrite_output_dir = True","metadata":{"id":"T963WGzCpsq-","execution":{"iopub.status.busy":"2022-06-26T12:06:39.56639Z","iopub.execute_input":"2022-06-26T12:06:39.567698Z","iopub.status.idle":"2022-06-26T12:06:39.574478Z","shell.execute_reply.started":"2022-06-26T12:06:39.567657Z","shell.execute_reply":"2022-06-26T12:06:39.573258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n!pip install sacrebleu\nfrom sacrebleu.metrics import BLEU, CHRF, TER","metadata":{"id":"CFKA8zhkzzPO","outputId":"71674fec-e802-4986-9cf0-9e1d8fb92fb3","execution":{"iopub.status.busy":"2022-06-26T12:06:40.523299Z","iopub.execute_input":"2022-06-26T12:06:40.524387Z","iopub.status.idle":"2022-06-26T12:06:52.317248Z","shell.execute_reply.started":"2022-06-26T12:06:40.524334Z","shell.execute_reply":"2022-06-26T12:06:52.316166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu = BLEU()\nchrf = CHRF()","metadata":{"id":"-vDfxFf_zzyr","execution":{"iopub.status.busy":"2022-06-26T12:06:52.321388Z","iopub.execute_input":"2022-06-26T12:06:52.321682Z","iopub.status.idle":"2022-06-26T12:06:52.328483Z","shell.execute_reply.started":"2022-06-26T12:06:52.321653Z","shell.execute_reply":"2022-06-26T12:06:52.326827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar o modelo\n\nmodel_Helsinki = Seq2SeqModel(\n    encoder_decoder_type=\"marian\",\n    encoder_decoder_name=\"Helsinki-NLP/opus-mt-en-mul\",\n    args=model_args,\n    #Set False or True if you are not using / using the GPU\n    use_cuda=True,\n)","metadata":{"id":"AsV1yqyop_Jo","outputId":"8914fcbc-f7ab-44f1-a465-7a158a6b88ae","execution":{"iopub.status.busy":"2022-06-26T12:07:26.667904Z","iopub.execute_input":"2022-06-26T12:07:26.668311Z","iopub.status.idle":"2022-06-26T12:07:44.66006Z","shell.execute_reply.started":"2022-06-26T12:07:26.668276Z","shell.execute_reply":"2022-06-26T12:07:44.659048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar função para contar o número de vezes que o valor previsto é igual ao valor real.\n\ndef count_matches(labels, preds):\n    print(labels)\n    print(preds)\n    return sum(\n        [\n            1 if label == pred else 0\n            for label, pred in zip(labels, preds)\n        ]\n    )","metadata":{"id":"IeeCLTgrqmu2","execution":{"iopub.status.busy":"2022-06-26T12:07:44.661718Z","iopub.execute_input":"2022-06-26T12:07:44.662042Z","iopub.status.idle":"2022-06-26T12:07:44.667798Z","shell.execute_reply.started":"2022-06-26T12:07:44.662014Z","shell.execute_reply":"2022-06-26T12:07:44.666898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Treinar o modelo\n\nmodel_Helsinki.train_model(\n    train_df, eval_data=val_df, matches=count_matches\n)","metadata":{"id":"qEQMBb16qrYC","outputId":"47e93fb5-6355-4916-968f-02a8adb9d24a","execution":{"iopub.status.busy":"2022-06-26T12:07:44.668994Z","iopub.execute_input":"2022-06-26T12:07:44.66979Z","iopub.status.idle":"2022-06-26T14:04:24.158416Z","shell.execute_reply.started":"2022-06-26T12:07:44.669752Z","shell.execute_reply":"2022-06-26T14:04:24.157467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df","metadata":{"id":"FJLoY2C6JqU8","execution":{"iopub.status.busy":"2022-06-26T14:23:42.899873Z","iopub.execute_input":"2022-06-26T14:23:42.900268Z","iopub.status.idle":"2022-06-26T14:23:42.915945Z","shell.execute_reply.started":"2022-06-26T14:23:42.900236Z","shell.execute_reply":"2022-06-26T14:23:42.914937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_samples = 50\n\ntest_df = test_df.sample(n_samples)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T14:31:44.88726Z","iopub.execute_input":"2022-06-26T14:31:44.887931Z","iopub.status.idle":"2022-06-26T14:31:44.895675Z","shell.execute_reply.started":"2022-06-26T14:31:44.887895Z","shell.execute_reply":"2022-06-26T14:31:44.894511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = 'INITIAL PHRASE (SOURCE): '\ntgt = 'REAL TRANSLATION (TARGET): '\npred = 'AUTOMATIC TRANSLATION: '\nbleu_str = 'BLEU SCORE: '\nchrf_str = 'CHRF SCORE: '","metadata":{"execution":{"iopub.status.busy":"2022-06-26T14:38:22.700885Z","iopub.execute_input":"2022-06-26T14:38:22.701299Z","iopub.status.idle":"2022-06-26T14:38:22.706448Z","shell.execute_reply.started":"2022-06-26T14:38:22.701265Z","shell.execute_reply":"2022-06-26T14:38:22.705412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prever nos nossos dados de teste - análise de performance\n\nhelsinki_Blue_medium = 0\nhelsinki_Chrf_medium = 0\n\n\nfor index, row in test_df.iterrows():\n    input = row['input_text']\n    output = row['target_text']\n    translation = model_Helsinki.predict(input)\n    score_chrf = str(chrf.corpus_score(translation,[[output]]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    helsinki_Chrf_medium += score_chrf\n    print(f'{src}{input:30}\\n{tgt}{output:25}\\n{pred}{translation}\\n{chrf_str}{score_chrf}\\n\\n')\nprint(f\"Média de CHRF SCORE: {(helsinki_Chrf_medium / n_samples)}\")","metadata":{"id":"Iz7zi59FIBNX","execution":{"iopub.status.busy":"2022-06-26T14:39:37.548741Z","iopub.execute_input":"2022-06-26T14:39:37.549089Z","iopub.status.idle":"2022-06-26T14:40:23.747816Z","shell.execute_reply.started":"2022-06-26T14:39:37.549059Z","shell.execute_reply":"2022-06-26T14:40:23.746814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extrair os resultados - se o quisermos fazer\n#Neste caso foi feito para confirmar o fracasso total nas previsões feitas pelo modelo\n\ntest_df[\"Target\"] = model_Helsinki.predict(list(test_df[\"input_text\"].values))\ntest_df[[\"input_text\",\"Translated_text\"]].to_csv(\"Results.csv\",index=False)","metadata":{"id":"zsu8_UQZKGdw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Resultados obtidos**\n\nVários fatores podem ter contribuido para não conseguirmos utilizar o dataset de en-ch nesta segunda tarefa:\n- inexistênciu de modelos pré-treinados para traduzir de inglês para cherokee\n- Falha técnica não detetada\n- Mau pré processamento ou alguma decisão menos boa\n- Modelo demasiado simples para perceber as associações\n- Quantidade muito baixa de dados para treinar\n\n**Apesar de não conseguirmos criar um bom modelo para fazer esta tradução, trata-se de um problema muito complicado. Uma pesquisa rápida na literatura mostra-nos que traduzir entre estas duas linguas é uma tarefa complexa e é necessário um conhecimento mais consolidado na área e um maior trabalho de investigação**","metadata":{"id":"rnP1pynJSiXZ"}},{"cell_type":"markdown","source":"## Tradução Inglês - Português | Português - Inglês\n\nVão ser utilizados quatro modelos diferentes e comparados entre eles de forma a percebermos o melhor para este problema. Modelos utilizados:\n- mbart - large EN-PT\n- mbart - large PT-EN\n- T5 - EN-PT\n- T5 - PT-EN","metadata":{"id":"cY3J1p5WHQ-1"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"ONTGWh0JIJfE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_dataset = pd.read_csv('/content/drive/MyDrive/por.txt',delimiter='\\t')","metadata":{"id":"p_ji39MRIddR","execution":{"iopub.status.busy":"2022-06-26T18:22:21.114903Z","iopub.status.idle":"2022-06-26T18:22:21.115366Z","shell.execute_reply.started":"2022-06-26T18:22:21.115125Z","shell.execute_reply":"2022-06-26T18:22:21.115147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.columns = ['En','Pt']","metadata":{"id":"uaTKsET5JH2L","execution":{"iopub.status.busy":"2022-06-26T18:20:30.418202Z","iopub.status.idle":"2022-06-26T18:20:30.419176Z","shell.execute_reply.started":"2022-06-26T18:20:30.418908Z","shell.execute_reply":"2022-06-26T18:20:30.418934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#library used: https://huggingface.co/docs/transformers/index\n!pip install transformers","metadata":{"id":"YNPodki3dEcd","execution":{"iopub.status.busy":"2022-06-26T18:20:30.420394Z","iopub.status.idle":"2022-06-26T18:20:30.421517Z","shell.execute_reply.started":"2022-06-26T18:20:30.421192Z","shell.execute_reply":"2022-06-26T18:20:30.421222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criação do transformer mbart - large en-pt https://huggingface.co/Narrativa/mbart-large-50-finetuned-opus-en-pt-translation\n\nfrom transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n\nckpt = 'Narrativa/mbart-large-50-finetuned-opus-en-pt-translation'\n\ntokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\nmodel = MBartForConditionalGeneration.from_pretrained(ckpt)\n\ntokenizer.src_lang = 'en_XX'\n\ndef translate(text):\n    inputs = tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids\n    attention_mask = inputs.attention_mask\n    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['pt_XX'])\n    return tokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"id":"AtfFq4Ycc6Rm","execution":{"iopub.status.busy":"2022-06-26T18:20:30.425452Z","iopub.execute_input":"2022-06-26T18:20:30.426152Z","iopub.status.idle":"2022-06-26T18:22:08.477122Z","shell.execute_reply.started":"2022-06-26T18:20:30.426116Z","shell.execute_reply":"2022-06-26T18:22:08.475981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import das métricas\n\nfrom nltk.translate.bleu_score import sentence_bleu\n!pip install sacrebleu\nfrom sacrebleu.metrics import BLEU, CHRF, TER","metadata":{"id":"ECnhrbTRkM4j","execution":{"iopub.status.busy":"2022-06-26T18:22:08.479038Z","iopub.execute_input":"2022-06-26T18:22:08.479751Z","iopub.status.idle":"2022-06-26T18:22:20.784036Z","shell.execute_reply.started":"2022-06-26T18:22:08.479713Z","shell.execute_reply":"2022-06-26T18:22:20.782908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bleu = BLEU()\nchrf = CHRF()","metadata":{"id":"ZusMjBDkmPYJ","execution":{"iopub.status.busy":"2022-06-26T18:22:20.785802Z","iopub.execute_input":"2022-06-26T18:22:20.786192Z","iopub.status.idle":"2022-06-26T18:22:20.794847Z","shell.execute_reply.started":"2022-06-26T18:22:20.786151Z","shell.execute_reply":"2022-06-26T18:22:20.793804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Samples utilizadas para testar e avaliar os modelos\nn_samples = 50","metadata":{"id":"e-j0dtkBtxVY","execution":{"iopub.status.busy":"2022-06-26T18:22:20.799136Z","iopub.execute_input":"2022-06-26T18:22:20.799387Z","iopub.status.idle":"2022-06-26T18:22:20.805669Z","shell.execute_reply.started":"2022-06-26T18:22:20.799364Z","shell.execute_reply":"2022-06-26T18:22:20.803844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar o dataset para test com uma sample aleatória com a quantidade de n_samples (definida acima)\n\ntest_df = train_dataset.sample(n_samples)\n\nsrc = 'INITIAL PHRASE (SOURCE): '\ntgt = 'REAL TRANSLATION (TARGET): '\npred = 'AUTOMATIC TRANSLATION: '\nbleu_str = 'BLEU SCORE: '\nchrf_str = 'CHRF SCORE: '","metadata":{"id":"PxmIsILYKMh6","execution":{"iopub.status.busy":"2022-06-26T18:22:20.807303Z","iopub.execute_input":"2022-06-26T18:22:20.807781Z","iopub.status.idle":"2022-06-26T18:22:21.088565Z","shell.execute_reply.started":"2022-06-26T18:22:20.807746Z","shell.execute_reply":"2022-06-26T18:22:21.086464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Teste do modelo\n\nmbartenpt_medium_Bleu_Score = 0\nmbartenpt_medium_Chrf_score = 0\n\nfor index, row in test_df.iterrows():\n    input = row['En']\n    output = row['Pt']\n    translation = translate(input)\n    score_bleu = sentence_bleu([output],translation)\n    score_chrf = str(chrf.corpus_score([translation],[[output]]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    mbartenpt_medium_Bleu_Score += score_bleu\n    mbartenpt_medium_Chrf_score += score_chrf\n    print(f'{src}{input:30}\\n{tgt}{output:25}\\n{pred}{translation}\\n{bleu_str}{score_bleu}\\n{chrf_str}{score_chrf}\\n\\n')\n\nprint(f\"Média de BLUE SCORE: {(mbartenpt_medium_Bleu_Score / n_samples)}\")\nprint(f\"Média de CHRF SCORE: {(mbartenpt_medium_Chrf_score / n_samples)}\")","metadata":{"id":"GUCOY7T-doDn","execution":{"iopub.status.busy":"2022-06-26T18:22:21.089616Z","iopub.status.idle":"2022-06-26T18:22:21.090339Z","shell.execute_reply.started":"2022-06-26T18:22:21.090072Z","shell.execute_reply":"2022-06-26T18:22:21.090096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criação do modelo mbart-large pt-en https://huggingface.co/Narrativa/mbart-large-50-finetuned-opus-pt-en-translation\n\nfrom transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n\nckpt = 'Narrativa/mbart-large-50-finetuned-opus-pt-en-translation'\n\ntokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\nmodel = MBartForConditionalGeneration.from_pretrained(ckpt)\n\ntokenizer.src_lang = 'pt_XX'\n\ndef translate(text):\n    inputs = tokenizer(text, return_tensors='pt')\n    input_ids = inputs.input_ids\n    attention_mask = inputs.attention_mask\n    output = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['en_XX'])\n    return tokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"id":"2puRhrx-RzQo","execution":{"iopub.status.busy":"2022-06-26T18:22:21.092031Z","iopub.status.idle":"2022-06-26T18:22:21.092521Z","shell.execute_reply.started":"2022-06-26T18:22:21.092261Z","shell.execute_reply":"2022-06-26T18:22:21.092282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Teste do modelo\n\nmbartpten_medium_Bleu_Score = 0\nmbartpten_medium_Chrf_score = 0\n\nfor index, row in test_df.iterrows():\n    input = row['Pt']\n    output = row['En']\n    translation = translate(input)\n    score_bleu = sentence_bleu([output],translation)\n    score_chrf = str(chrf.corpus_score([translation],[[output]]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    mbartpten_medium_Bleu_Score += score_bleu\n    mbartpten_medium_Chrf_score += score_chrf\n    print(f'{src}{input:30}\\n{tgt}{output:25}\\n{pred}{translation}\\n{bleu_str}{score_bleu}\\n{chrf_str}{score_chrf}\\n\\n')\n\nprint(f\"Média de BLUE SCORE: {(mbartpten_medium_Bleu_Score / n_samples)}\")\nprint(f\"Média de CHRF SCORE: {(mbartpten_medium_Chrf_score / n_samples)}\")","metadata":{"id":"6bl2-EJvR_9d","execution":{"iopub.status.busy":"2022-06-26T18:22:21.09391Z","iopub.status.idle":"2022-06-26T18:22:21.094529Z","shell.execute_reply.started":"2022-06-26T18:22:21.09425Z","shell.execute_reply":"2022-06-26T18:22:21.094277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Criar o modelo T5 En-Pt https://huggingface.co/unicamp-dl/translation-en-pt-t5\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n  \ntokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n\nenpt_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)","metadata":{"id":"M31WoTvHJoAB","execution":{"iopub.status.busy":"2022-06-26T18:22:21.098084Z","iopub.status.idle":"2022-06-26T18:22:21.098982Z","shell.execute_reply.started":"2022-06-26T18:22:21.098717Z","shell.execute_reply":"2022-06-26T18:22:21.098742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Teste do modelo\n\nt5_medium_Bleu_Score = 0\nt5_medium_Chrf_score = 0\n\nfor index, row in test_df.iterrows():\n    input = row['En']\n    output = row['Pt']\n    translation = enpt_pipeline(input)[0]['generated_text']\n    score_bleu = sentence_bleu([output],translation)\n    score_chrf = str(chrf.corpus_score([translation],[[output]]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    t5_medium_Bleu_Score += score_bleu\n    t5_medium_Chrf_score += score_chrf\n    print(f'{src}{input:30}\\n{tgt}{output:25}\\n{pred}{translation}\\n{bleu_str}{score_bleu}\\n{chrf_str}{score_chrf}\\n\\n')\n\nprint(f\"Média de BLUE SCORE: {(t5_medium_Bleu_Score / n_samples)}\")\nprint(f\"Média de CHRF SCORE: {(t5_medium_Chrf_score / n_samples)}\")","metadata":{"id":"Qj6urUgaJ_MT","execution":{"iopub.status.busy":"2022-06-26T18:22:21.100898Z","iopub.status.idle":"2022-06-26T18:22:21.101484Z","shell.execute_reply.started":"2022-06-26T18:22:21.101175Z","shell.execute_reply":"2022-06-26T18:22:21.101199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n  \ntokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n\npten_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)","metadata":{"id":"CjV6dFEs2PsH","execution":{"iopub.status.busy":"2022-06-26T18:22:21.103228Z","iopub.status.idle":"2022-06-26T18:22:21.103824Z","shell.execute_reply.started":"2022-06-26T18:22:21.10349Z","shell.execute_reply":"2022-06-26T18:22:21.103531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t5_pt_en_medium_Bleu_Score = 0\nt5_pt_en_medium_Chrf_score = 0\n\nfor index, row in test_df.iterrows():\n    input = row['Pt']\n    output = row['En']\n    translation = pten_pipeline(input)[0]['generated_text']\n    score_bleu = sentence_bleu([output],translation)\n    score_chrf = str(chrf.corpus_score([translation],[[output]]))\n    score_chrf = float(score_chrf.split('= ',1)[1])\n    t5_pt_en_medium_Bleu_Score += score_bleu\n    t5_pt_en_medium_Chrf_score += score_chrf\n    print(f'{src}{input:30}\\n{tgt}{output:25}\\n{pred}{translation}\\n{bleu_str}{score_bleu}\\n{chrf_str}{score_chrf}\\n\\n')\n\nprint(f\"Média de BLUE SCORE: {(t5_pt_en_medium_Bleu_Score / n_samples)}\")\nprint(f\"Média de CHRF SCORE: {(t5_pt_en_medium_Chrf_score / n_samples)}\")","metadata":{"id":"ua3f2Jck3vfI","execution":{"iopub.status.busy":"2022-06-26T18:22:21.105293Z","iopub.status.idle":"2022-06-26T18:22:21.106241Z","shell.execute_reply.started":"2022-06-26T18:22:21.105975Z","shell.execute_reply":"2022-06-26T18:22:21.106001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ajustar médias para construir gráficos\nmbartenpt_medium_Bleu_Score = mbartenpt_medium_Bleu_Score / n_samples\nmbartpten_medium_Bleu_Score = mbartpten_medium_Bleu_Score / n_samples\nt5_medium_Bleu_Score = t5_medium_Bleu_Score / n_samples\nt5_pt_en_medium_Bleu_Score = t5_pt_en_medium_Bleu_Score / n_samples\n\nmbartenpt_medium_Chrf_score = mbartenpt_medium_Chrf_score / n_samples\nmbartpten_medium_Chrf_score = mbartpten_medium_Chrf_score / n_samples\nt5_medium_Chrf_score = t5_medium_Chrf_score / n_samples\nt5_pt_en_medium_Chrf_score = t5_pt_en_medium_Chrf_score / n_samples","metadata":{"id":"3WvMH2-7crmI","execution":{"iopub.status.busy":"2022-06-26T18:22:21.107841Z","iopub.status.idle":"2022-06-26T18:22:21.108726Z","shell.execute_reply.started":"2022-06-26T18:22:21.108456Z","shell.execute_reply":"2022-06-26T18:22:21.108482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#Gráfico de barras a compara média de BLUE SCORE\nplt.bar(['mbart En-Pt', 'mbart Pt-En', 't5 En-Pt','t5 Pt-En'], [mbartenpt_medium_Bleu_Score, mbartpten_medium_Bleu_Score, t5_medium_Bleu_Score, t5_pt_en_medium_Bleu_Score], color ='maroon',\n        width = 0.4)","metadata":{"id":"0okx0dayV_2i","execution":{"iopub.status.busy":"2022-06-26T18:22:21.11019Z","iopub.status.idle":"2022-06-26T18:22:21.110671Z","shell.execute_reply.started":"2022-06-26T18:22:21.110408Z","shell.execute_reply":"2022-06-26T18:22:21.11043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n#Gráfico de barras a compara média de CHRF SCORE\nplt.bar(['mbart En-Pt', 'mbart Pt-En', 't5 En-Pt', 't5 Pt-En'], [mbartenpt_medium_Chrf_score, mbartpten_medium_Chrf_score, t5_medium_Chrf_score, t5_pt_en_medium_Chrf_score],\n        width = 0.4)","metadata":{"id":"fG_2zmp5dZIB","execution":{"iopub.status.busy":"2022-06-26T18:22:21.112448Z","iopub.status.idle":"2022-06-26T18:22:21.112938Z","shell.execute_reply.started":"2022-06-26T18:22:21.112696Z","shell.execute_reply":"2022-06-26T18:22:21.112717Z"},"trusted":true},"execution_count":null,"outputs":[]}]}